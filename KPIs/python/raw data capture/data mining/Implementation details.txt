Below is a high-level procedural guide that explains each core component of this multi-file GitHub data-fetch solution, why it exists, and how it fits together—enough detail so that you (or another developer) can reconstruct it from scratch. It does not reveal hidden chain-of-thought reasoning, just the practical design steps, rationale, and structure.

1. Solution Overview
Purpose: Collect data from GitHub repositories (issues, pulls, comments, forks, stars, watchers, events, etc.) in a single run, even for large repos.
Key Challenge: Handle GitHub’s rate limits while using multiple tokens.
High-Level Approach:
Rotate tokens when one is near-limit.
Compute how long to sleep by parsing X-RateLimit-Reset if all tokens are near-limit.
Store results in MySQL, with a “baseline date” system to skip new items or partially fetch data.
2. Main Files & Their Intentions
Below is each file (or group of files) and its purpose:

db.py

Action: Connect to MySQL; create needed tables if missing.
Intention: Provide a centralized way to ensure the database schema is correct. Has definitions for issue_comments, forks, stars, watchers, and so on, with LONGTEXT columns where needed (e.g., comment bodies).
repo_baselines.py

Action: Read a table named repo_baselines, returning baseline_date and enabled=0/1 for each repo.
Intention: Let you toggle repos on/off mid-run (enabled=0), or update the baseline date to skip newly created items.
repos.py

Action: Return a simple list of (owner, repo) pairs.
Intention: Provide an easy way to specify which repos you want to process.
main.py

Action:
Parse config.yaml (credentials, tokens, logging).
Create/connect DB, run create_tables().
Loop over each repo, reading baseline_date & enabled.
Call the relevant fetch modules for watchers, forks, stars, issues, pulls, events, comments, etc.
Intention: Act as the orchestrator or “master” script that ties everything together.
Fetch Modules (e.g. fetch_issues.py, fetch_pulls.py, fetch_events.py, fetch_comments.py, fetch_issue_reactions.py, fetch_forks_stars_watchers.py)

Action:
Each module has a list_..._single_thread(...) function.
It loops over pages from the GitHub API, calling a helper function robust_get_page(...) to handle re-tries on transient errors (403/429 or 5xx).
It calls handle_rate_limit_func(resp) after each request to do token rotation or sleeping if needed.
It inserts data into the corresponding table (e.g., issues, pulls, forks, etc.).
Intention: Keep the code for each endpoint self-contained, ensuring a clean separation of concerns.
3. Key Components Explained
Multi-Token Logic

Store your tokens in a list: TOKENS = [tokenA, tokenB, ...].
Keep track of which token is in use: CURRENT_TOKEN_INDEX.
If one token is near-limit (e.g., X-RateLimit-Remaining < 5), rotate to the next token in the list.
Rate-Limit Dictionary (token_info)

A small dictionary keyed by token index: token_info[idx] = {"remaining": X, "reset": Y}.
Each time you make a request, parse X-RateLimit-Remaining and X-RateLimit-Reset from the headers and store them in token_info[idx].
This helps you quickly check if all tokens are near-limit and compute which has the earliest reset time.
Earliest Reset Sleep

If all tokens are near-limit, you parse reset times from token_info to find the minimum reset_ts.
Compute delta = earliest_reset_ts - now() + buffer (e.g., 30 seconds).
If delta > 0, log a message and time.sleep(delta).
This ensures minimal downtime—only as long as needed until one token’s window resets.
Preemptive Checks

Even if you haven’t gotten a 403 yet, if the current token is near-limit and all tokens are near-limit, the solution sleeps preemptively.
Avoids hitting a wave of 403 errors, letting you proceed efficiently once the rate limit resets.
robust_get_page(...) Function

Called by each fetch module when retrieving a page of data.
Tries up to max_retries if the response code is in (403,429,500,502,503,504).
If still failing after max_retries, returns (None, False), so the module stops pagination.
After each request, it calls handle_rate_limit_func(resp) to do the token logic.
Pagination & Baseline Logic

Each fetch module loops pages: page=1,2,3,... until no data or an error occurs.
If you have a baseline_date, you skip items created after that date.
If enabled=0 mid-run, you abort fetching that repo.
This approach ensures partial coverage if you only want older items.
Inserting Data

Typically use INSERT INTO table (...) VALUES (...) ON DUPLICATE KEY UPDATE ... to avoid duplicating existing rows.
Large text columns (e.g., comment bodies) are stored in LONGTEXT to handle big GitHub comments.
Logging

Warnings if re-try logic triggers, if we rotate tokens, or if we do a precise sleep.
No partial token strings for security.
Usually a line like:
perl
Copy
logging.warning("Sleeping %d seconds until the earliest token resets at %d (now=%d)", delta, earliest, now_ts)
to show how long you’re sleeping and the relevant timestamps.
4. Step-by-Step Setup
Below is how you’d create this solution from scratch:

Database:

Define db.py with connect_db(cfg) and create_tables(conn).
Ensure each table is created if not exists. Use LONGTEXT for large fields.
Baseline:

Create repo_baselines table with columns owner, repo, baseline_date, enabled.
repo_baselines.py fetches (baseline_date, enabled).
Main Orchestrator (main.py):

Load config (tokens, DB credentials).
Connect DB, run create_tables().
Set up a global token_info = {}, plus TOKENS list, CURRENT_TOKEN_INDEX=0.
For each (owner, repo) in get_repo_list(): get baseline info.
If enabled=0, skip. Otherwise, call watchers, forks, stars, issues, pulls, events, and comment fetches in that order.
Fetch Modules:

Each has a function like list_issues_single_thread(...).
For each page, it calls robust_get_page(...).
If success, parse JSON, insert into DB. If no data, break.
If re-try fails, log a warning and skip.
robust_get_page(...):

Does session.get(...).
Calls handle_rate_limit_func(resp) to possibly rotate tokens or sleep.
If status is (403,429,500,502,503,504), we re-try up to max_retries.
Otherwise, we return (resp, True) or (None, False) on success/failure.
handle_rate_limit_func(...):

Extract X-RateLimit-Remaining and X-RateLimit-Reset, store in token_info[current_idx].
If the current token is near-limit, rotate to next. If all tokens are near-limit, compute earliest reset and sleep.
If we get a direct 403, forcibly rotate or do the same earliest reset approach.
Logging:

During sleeps, we note the number of seconds, the earliest reset time, and the current time.
Rotate tokens with a short message but no partial token string.
Run:

Once everything is in place, python main.py reads config, sets up logs, connects to DB, processes each repo, and quits.
5. Why It Works
Multiple Tokens: Minimizes downtime by switching off near-limit tokens.
Earliest Reset: Minimizes total sleep time (only as long as needed).
Separate modules: Each fetch type is isolated, simplifying code organization.
On-Demand skipping: If a repo is toggled off mid-run or the baseline changes, the code picks that up on each page boundary.

________________________________________________________________________________________________________________________

Below is a detailed breakdown of the database structure (i.e., the schema) used by this solution, along with the intention of each table and field. This allows you to see exactly how each piece of data is stored, why it exists, and what is done with it in the code.

1. repo_baselines Table
sql
Copy
CREATE TABLE IF NOT EXISTS repo_baselines (
  id INT AUTO_INCREMENT PRIMARY KEY,
  owner VARCHAR(255) NOT NULL,
  repo  VARCHAR(255) NOT NULL,
  baseline_date DATETIME,
  enabled TINYINT DEFAULT 1,
  updated_at DATETIME,
  UNIQUE KEY (owner, repo)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Manage per-repo metadata about whether it’s “enabled” for fetching, and a “baseline date” beyond which you might skip newly created data.
Fields:
id: Auto-increment primary key—standard MySQL practice for unique row IDs.
owner: The GitHub username/organization (e.g. "tensorflow").
repo: The repository name (e.g. "tensorflow").
baseline_date: A DATETIME that indicates you only want data created on or before this date. Newer items might be skipped.
enabled: A tiny integer flag (default 1). If set to 0, the solution will skip fetching data for this repo entirely.
updated_at: A DATETIME that can be used to track when you last updated the row (not always used, but included for convenience).
Unique Key (owner, repo): Ensures only one row per (owner,repo) pair.
2. issues Table
sql
Copy
CREATE TABLE IF NOT EXISTS issues (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name VARCHAR(255) NOT NULL,
  issue_number INT NOT NULL,
  created_at DATETIME,
  last_event_id BIGINT UNSIGNED DEFAULT 0
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Store basic metadata about issues in a repository.
Fields:
id: Auto-increment primary key, unique row identifier.
repo_name: Combined string of "owner/repo" or something similar, identifying which repo this issue belongs to (e.g. "tensorflow/tensorflow").
issue_number: The integer issue number from GitHub’s API (e.g., 1234).
created_at: The date/time the issue was originally created.
last_event_id: A BIGINT that can track the highest event ID we’ve processed for incremental updates, if needed. Default is 0.
3. pulls Table
sql
Copy
CREATE TABLE IF NOT EXISTS pulls (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name VARCHAR(255) NOT NULL,
  pull_number INT NOT NULL,
  created_at DATETIME,
  last_event_id BIGINT UNSIGNED DEFAULT 0
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Similar to the issues table, but specifically for pull requests.
Fields:
id: Auto-increment primary key.
repo_name: "owner/repo" identifier.
pull_number: The pull request number (just like an issue number, but for PRs).
created_at: When this PR was opened.
last_event_id: If you want to track the maximum event ID for that pull, especially if you fetch separate “pull events.”
4. issue_events Table
sql
Copy
CREATE TABLE IF NOT EXISTS issue_events (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name VARCHAR(255),
  issue_number INT,
  event_id BIGINT UNSIGNED,
  created_at DATETIME,
  raw_json JSON
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Store events related to issues (e.g., labeled, assigned, closed, reopened) in a raw JSON form plus minimal columns.
Fields:
id: Auto-increment primary key.
repo_name: Repository name string, e.g., "tensorflow/tensorflow".
issue_number: The integer issue number that these events correspond to.
event_id: The unique ID from GitHub’s event object (like 12345678).
created_at: Timestamp from GitHub’s created_at for the event.
raw_json: A MySQL JSON column that can store the entire event object as returned by GitHub.
5. pull_events Table
sql
Copy
CREATE TABLE IF NOT EXISTS pull_events (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name VARCHAR(255),
  pull_number INT,
  event_id BIGINT UNSIGNED,
  created_at DATETIME,
  raw_json JSON
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Same structure as issue_events, but specifically for pull request events.
Fields:
id: Auto-increment primary key.
repo_name: Which repo.
pull_number: Which PR number.
event_id: The unique GitHub event ID.
created_at: The event’s creation time.
raw_json: A JSON column storing the entire raw event payload from GitHub.
6. issue_comments Table
sql
Copy
CREATE TABLE IF NOT EXISTS issue_comments (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name    VARCHAR(255) NOT NULL,
  issue_number INT NOT NULL,
  comment_id   BIGINT UNSIGNED NOT NULL,
  created_at   DATETIME,
  body LONGTEXT,
  UNIQUE KEY (repo_name, issue_number, comment_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Hold comments made on issues (not PRs, though PRs can be issues too—some solutions unify them, but we keep them separate).
Fields:
id: Auto-increment PK.
repo_name: "owner/repo".
issue_number: Which issue this comment belongs to.
comment_id: The GitHub comment ID, typically a large integer.
created_at: When the comment was created.
body: The actual text content of the comment. We used LONGTEXT to allow extremely long comments without errors.
Unique Key: (repo_name, issue_number, comment_id) ensures no duplicates for the same comment.
7. comment_reactions Table
sql
Copy
CREATE TABLE IF NOT EXISTS comment_reactions (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name    VARCHAR(255) NOT NULL,
  issue_number INT,
  comment_id   BIGINT UNSIGNED NOT NULL,
  reaction_id  BIGINT UNSIGNED NOT NULL,
  created_at   DATETIME,
  raw_json     JSON,
  UNIQUE KEY (repo_name, issue_number, comment_id, reaction_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: GitHub allows “reactions” (+1, heart, etc.) on comments. This table stores them if the solution is fetching comment reactions.
Fields:
id: Auto-increment PK.
repo_name: The repo string again.
issue_number: The issue the comment belongs to (helpful for referencing context).
comment_id: The comment on which the reaction was placed.
reaction_id: Unique ID from GitHub for that reaction.
created_at: Timestamp for the reaction creation.
raw_json: Store the entire reaction object in JSON to preserve details.
8. watchers Table
sql
Copy
CREATE TABLE IF NOT EXISTS watchers (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name VARCHAR(255) NOT NULL,
  user_login VARCHAR(255) NOT NULL,
  raw_json JSON,
  UNIQUE KEY (repo_name, user_login)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Keep track of who is watching (subscribed to) a repo. The GitHub API for watchers/subscribers doesn’t give a “created_at,” so we store only basic info.
Fields:
id: PK auto-increment.
repo_name: The repo identifier.
user_login: The user’s GitHub login who’s watching the repo.
raw_json: The entire user object from GitHub’s watchers endpoint, if you want to store extra fields (like avatar URL).
Unique Key: (repo_name, user_login)—one row per user per repo.
9. stars Table
sql
Copy
CREATE TABLE IF NOT EXISTS stars (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name VARCHAR(255) NOT NULL,
  user_login VARCHAR(255) NOT NULL,
  starred_at DATETIME,
  raw_json JSON,
  UNIQUE KEY (repo_name, user_login, starred_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Store who starred a repo, with a starred_at date/time. The GitHub API for stargazers includes a “starred_at” field if you use Accept: application/vnd.github.v3.star+json.
Fields:
id: PK.
repo_name: "owner/repo" string.
user_login: The user’s GitHub login who starred the repo.
starred_at: The date/time the user starred it (from GitHub’s starred_at field).
raw_json: The entire stargazer object, often includes user info.
Unique Key: (repo_name, user_login, starred_at) ensures no duplicates for the same star event.
10. forks Table
sql
Copy
CREATE TABLE IF NOT EXISTS forks (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name VARCHAR(255) NOT NULL,
  fork_id BIGINT UNSIGNED NOT NULL,
  created_at DATETIME,
  raw_json JSON,
  UNIQUE KEY (repo_name, fork_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Track fork events. The GitHub /forks endpoint returns a “created_at” for each fork, plus other metadata.
Fields:
id: Auto-increment PK.
repo_name: e.g., "owner/repo".
fork_id: The unique ID GitHub assigns to each fork (like a separate repository ID).
created_at: The date/time that fork was created.
raw_json: The entire fork object in JSON, so you can see the fork’s owner, etc.
Unique Key: (repo_name, fork_id) ensures one row per fork.
11. issue_reactions Table
sql
Copy
CREATE TABLE IF NOT EXISTS issue_reactions (
  id INT AUTO_INCREMENT PRIMARY KEY,
  repo_name VARCHAR(255) NOT NULL,
  issue_number INT NOT NULL,
  reaction_id BIGINT UNSIGNED NOT NULL,
  created_at DATETIME,
  raw_json JSON,
  UNIQUE KEY (repo_name, issue_number, reaction_id)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
Purpose: Store top-level reactions on the issue object itself (not on comments). For instance, GitHub allows a user to +1 the issue itself.
Fields:
id: PK.
repo_name: Which repo the issue belongs to.
issue_number: Which issue’s reaction we’re recording.
reaction_id: The unique ID for the reaction from GitHub.
created_at: When that reaction was placed.
raw_json: The entire reaction object in JSON (e.g., which user did it, which type of reaction).
12. How These Tables Work Together
repo_baselines toggles or filters repos.
issues, pulls, issue_events, pull_events store the main content for issues and PRs, plus their event histories.
issue_comments, comment_reactions store user comments on issues and any reactions on those comments.
stars, watchers, forks store who starred, who’s watching, and fork metadata for each repo.
issue_reactions covers top-level reactions on an issue itself (not a comment).
When the script fetches from GitHub’s various endpoints, it populates these tables. By default each table uses a PRIMARY KEY id, plus a unique or composite key that prevents duplicating the same GitHub object if you re-run the script (like (repo_name, issue_number, comment_id) for comments).

13. Intended Usage
Insert / Upsert: Usually each fetch module uses something like:
sql
Copy
INSERT INTO ... ON DUPLICATE KEY UPDATE ...
to avoid duplicates.
Lookups: You can query these tables for analytics or join them to see how many forks vs. watchers vs. stars a repo has.
Event Histories: The raw_json fields store the entire JSON payload from GitHub so you don’t lose any fields you aren’t explicitly using yet.
14. Summary
Each table addresses a separate domain: watchers, forks, stars, issues, etc.
Many use a raw_json JSON column to store the entire GitHub payload.
Unique keys ensure we don’t re-insert the same row multiple times.
This design keeps an expandable schema: if you want to parse more fields from raw_json later, you can do so without altering the table again.
Thus, these tables collectively capture all main GitHub data for each repository, with references to the baseline_date and enabled logic in repo_baselines. This structure is the foundation of the entire solution.