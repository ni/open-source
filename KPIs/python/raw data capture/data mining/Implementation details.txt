Below is a high-level procedural guide that explains each core component of this multi-file GitHub data-fetch solution, why it exists, and how it fits together—enough detail so that you (or another developer) can reconstruct it from scratch. It does not reveal hidden chain-of-thought reasoning, just the practical design steps, rationale, and structure.

1. Solution Overview
Purpose: Collect data from GitHub repositories (issues, pulls, comments, forks, stars, watchers, events, etc.) in a single run, even for large repos.
Key Challenge: Handle GitHub’s rate limits while using multiple tokens.
High-Level Approach:
Rotate tokens when one is near-limit.
Compute how long to sleep by parsing X-RateLimit-Reset if all tokens are near-limit.
Store results in MySQL, with a “baseline date” system to skip new items or partially fetch data.
2. Main Files & Their Intentions
Below is each file (or group of files) and its purpose:

db.py

Action: Connect to MySQL; create needed tables if missing.
Intention: Provide a centralized way to ensure the database schema is correct. Has definitions for issue_comments, forks, stars, watchers, and so on, with LONGTEXT columns where needed (e.g., comment bodies).
repo_baselines.py

Action: Read a table named repo_baselines, returning baseline_date and enabled=0/1 for each repo.
Intention: Let you toggle repos on/off mid-run (enabled=0), or update the baseline date to skip newly created items.
repos.py

Action: Return a simple list of (owner, repo) pairs.
Intention: Provide an easy way to specify which repos you want to process.
main.py

Action:
Parse config.yaml (credentials, tokens, logging).
Create/connect DB, run create_tables().
Loop over each repo, reading baseline_date & enabled.
Call the relevant fetch modules for watchers, forks, stars, issues, pulls, events, comments, etc.
Intention: Act as the orchestrator or “master” script that ties everything together.
Fetch Modules (e.g. fetch_issues.py, fetch_pulls.py, fetch_events.py, fetch_comments.py, fetch_issue_reactions.py, fetch_forks_stars_watchers.py)

Action:
Each module has a list_..._single_thread(...) function.
It loops over pages from the GitHub API, calling a helper function robust_get_page(...) to handle re-tries on transient errors (403/429 or 5xx).
It calls handle_rate_limit_func(resp) after each request to do token rotation or sleeping if needed.
It inserts data into the corresponding table (e.g., issues, pulls, forks, etc.).
Intention: Keep the code for each endpoint self-contained, ensuring a clean separation of concerns.
3. Key Components Explained
Multi-Token Logic

Store your tokens in a list: TOKENS = [tokenA, tokenB, ...].
Keep track of which token is in use: CURRENT_TOKEN_INDEX.
If one token is near-limit (e.g., X-RateLimit-Remaining < 5), rotate to the next token in the list.
Rate-Limit Dictionary (token_info)

A small dictionary keyed by token index: token_info[idx] = {"remaining": X, "reset": Y}.
Each time you make a request, parse X-RateLimit-Remaining and X-RateLimit-Reset from the headers and store them in token_info[idx].
This helps you quickly check if all tokens are near-limit and compute which has the earliest reset time.
Earliest Reset Sleep

If all tokens are near-limit, you parse reset times from token_info to find the minimum reset_ts.
Compute delta = earliest_reset_ts - now() + buffer (e.g., 30 seconds).
If delta > 0, log a message and time.sleep(delta).
This ensures minimal downtime—only as long as needed until one token’s window resets.
Preemptive Checks

Even if you haven’t gotten a 403 yet, if the current token is near-limit and all tokens are near-limit, the solution sleeps preemptively.
Avoids hitting a wave of 403 errors, letting you proceed efficiently once the rate limit resets.
robust_get_page(...) Function

Called by each fetch module when retrieving a page of data.
Tries up to max_retries if the response code is in (403,429,500,502,503,504).
If still failing after max_retries, returns (None, False), so the module stops pagination.
After each request, it calls handle_rate_limit_func(resp) to do the token logic.
Pagination & Baseline Logic

Each fetch module loops pages: page=1,2,3,... until no data or an error occurs.
If you have a baseline_date, you skip items created after that date.
If enabled=0 mid-run, you abort fetching that repo.
This approach ensures partial coverage if you only want older items.
Inserting Data

Typically use INSERT INTO table (...) VALUES (...) ON DUPLICATE KEY UPDATE ... to avoid duplicating existing rows.
Large text columns (e.g., comment bodies) are stored in LONGTEXT to handle big GitHub comments.
Logging

Warnings if re-try logic triggers, if we rotate tokens, or if we do a precise sleep.
No partial token strings for security.
Usually a line like:
perl
Copy
logging.warning("Sleeping %d seconds until the earliest token resets at %d (now=%d)", delta, earliest, now_ts)
to show how long you’re sleeping and the relevant timestamps.
4. Step-by-Step Setup
Below is how you’d create this solution from scratch:

Database:

Define db.py with connect_db(cfg) and create_tables(conn).
Ensure each table is created if not exists. Use LONGTEXT for large fields.
Baseline:

Create repo_baselines table with columns owner, repo, baseline_date, enabled.
repo_baselines.py fetches (baseline_date, enabled).
Main Orchestrator (main.py):

Load config (tokens, DB credentials).
Connect DB, run create_tables().
Set up a global token_info = {}, plus TOKENS list, CURRENT_TOKEN_INDEX=0.
For each (owner, repo) in get_repo_list(): get baseline info.
If enabled=0, skip. Otherwise, call watchers, forks, stars, issues, pulls, events, and comment fetches in that order.
Fetch Modules:

Each has a function like list_issues_single_thread(...).
For each page, it calls robust_get_page(...).
If success, parse JSON, insert into DB. If no data, break.
If re-try fails, log a warning and skip.
robust_get_page(...):

Does session.get(...).
Calls handle_rate_limit_func(resp) to possibly rotate tokens or sleep.
If status is (403,429,500,502,503,504), we re-try up to max_retries.
Otherwise, we return (resp, True) or (None, False) on success/failure.
handle_rate_limit_func(...):

Extract X-RateLimit-Remaining and X-RateLimit-Reset, store in token_info[current_idx].
If the current token is near-limit, rotate to next. If all tokens are near-limit, compute earliest reset and sleep.
If we get a direct 403, forcibly rotate or do the same earliest reset approach.
Logging:

During sleeps, we note the number of seconds, the earliest reset time, and the current time.
Rotate tokens with a short message but no partial token string.
Run:

Once everything is in place, python main.py reads config, sets up logs, connects to DB, processes each repo, and quits.
5. Why It Works
Multiple Tokens: Minimizes downtime by switching off near-limit tokens.
Earliest Reset: Minimizes total sleep time (only as long as needed).
Separate modules: Each fetch type is isolated, simplifying code organization.
On-Demand skipping: If a repo is toggled off mid-run or the baseline changes, the code picks that up on each page boundary.