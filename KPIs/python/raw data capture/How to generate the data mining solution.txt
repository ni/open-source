Below is your final three-file solution, where we:

Create a new database each run (using a prefix + timestamp).
Use the exact fetch logic from your reference script (with all debug prints, page handling, rate-limit calls, etc.) merged with the consecutive-empty approach for chunk-based forks/stars.
Implement “incremental” logic (?since=...) for issues/pulls, though each run is effectively fresh since the DB is newly created.
Everything is fully pasted into the fetch_*_data functions, preserving your debug statements.

1) repo_list.py
python
Copy code
# repo_list.py
"""
Each repo dict has:
  - enabled (bool)
  - owner   (str)
  - repo    (str)
  - start_date (str 'YYYY-MM-DD')
  - end_date   (str 'YYYY-MM-DD' or ""), meaning up to 'now'
"""

repo_list = [
    {
        "enabled": True,
        "owner": "ni",
        "repo": "grpc-labview",
        "start_date": "2007-11-02",
        "end_date": ""
    },
    {
        "enabled": True,
        "owner": "facebook",
        "repo": "react",
        "start_date": "2007-11-02",
        "end_date": ""
    },
    {
        "enabled": True,
        "owner": "tensorflow",
        "repo": "tensorflow",
        "start_date": "2007-11-02",
        "end_date": ""
    }
    # ... add more if needed ...
]
2) fetch_data.py
Below is the complete script with:

create_and_select_db() to make a new DB each run.
Tables built in create_tables().
Chunk-based logic for fetch_fork_data and fetch_star_data, including consecutive-empty logic and all debug prints from your reference script.
Incremental logic for fetch_issue_data and fetch_pull_data, also with your debug statements.
Robust retry logic in get_session().
python
Copy code
#!/usr/bin/env python
# fetch_data.py

import os
import re
import requests
import mysql.connector
from datetime import datetime, timedelta
from time import sleep

from requests.adapters import HTTPAdapter, Retry

############################
# DB prefix => environment variable or default
############################
DB_PREFIX = os.getenv("DB_PREFIX", "my_kpis_db_")
CURRENT_DB_NAME = None  # We'll set this at runtime

############################
# GitHub token logic
############################
TOKENS = []
CURRENT_TOKEN_INDEX = 0
MAX_LIMIT_BUFFER = 50

def load_tokens():
    """
    Load tokens from 'tokens.txt' or env variables (GITHUB_TOKEN1, GITHUB_TOKEN2).
    If none => run unauthenticated => lower rate limits.
    """
    global TOKENS
    script_dir = os.path.dirname(os.path.abspath(__file__))
    tokens_file = os.path.join(script_dir, "tokens.txt")
    if os.path.isfile(tokens_file):
        with open(tokens_file, "r", encoding="utf-8") as f:
            lines = [ln.strip() for ln in f.read().splitlines() if ln.strip()]
            TOKENS = lines
    else:
        t1 = os.getenv("GITHUB_TOKEN1", "")
        t2 = os.getenv("GITHUB_TOKEN2", "")
        TOKENS = [tk for tk in [t1, t2] if tk]

    if TOKENS:
        print(f"Loaded {len(TOKENS)} GitHub token(s).")
    else:
        print("No GitHub tokens found => unauthenticated => lower rate limits.")

def create_unique_db_name():
    """
    Generate a new DB name using DB_PREFIX + a timestamp,
    e.g. "my_kpis_db_2023_08_21_135500"
    """
    timestamp_str = datetime.now().strftime("%Y_%m_%d_%H%M%S")
    return f"{DB_PREFIX}{timestamp_str}"

def connect_server_no_db():
    """
    Connect to MySQL at the server level (no DB specified),
    so we can CREATE a new DB.
    """
    return mysql.connector.connect(
        host=os.getenv("DB_HOST", "localhost"),
        user=os.getenv("DB_USER", "root"),
        password=os.getenv("DB_PASS", "root")
    )

def create_and_select_db():
    """
    Create a brand-new DB name, store in CURRENT_DB_NAME.
    """
    global CURRENT_DB_NAME
    db_name = create_unique_db_name()

    conn = connect_server_no_db()
    c = conn.cursor()

    print(f"Creating new database: {db_name}")
    c.execute(f"CREATE DATABASE IF NOT EXISTS `{db_name}`")
    conn.commit()
    c.close()
    conn.close()

    CURRENT_DB_NAME = db_name

def connect_db():
    """
    Connect to the newly created DB: CURRENT_DB_NAME
    """
    global CURRENT_DB_NAME
    if not CURRENT_DB_NAME:
        raise Exception("CURRENT_DB_NAME is not set. Must call create_and_select_db first.")

    return mysql.connector.connect(
        host=os.getenv("DB_HOST", "localhost"),
        user=os.getenv("DB_USER", "root"),
        password=os.getenv("DB_PASS", "root"),
        database=CURRENT_DB_NAME
    )

def create_tables():
    """
    Create ephemeral tables in this new DB => forks, pulls, issues, stars
    """
    conn = connect_db()
    c = conn.cursor()

    c.execute("""
    CREATE TABLE forks (
        repo_name VARCHAR(255) NOT NULL,
        creator_login VARCHAR(255),
        forked_at DATETIME NOT NULL,
        PRIMARY KEY (repo_name, forked_at, creator_login)
    ) ENGINE=InnoDB
    """)

    c.execute("""
    CREATE TABLE pulls (
        repo_name VARCHAR(255) NOT NULL,
        pr_number INT NOT NULL,
        created_at DATETIME NOT NULL,
        first_review_at DATETIME,
        merged_at DATETIME,
        creator_login VARCHAR(255),
        title TEXT,
        updated_at DATETIME,
        PRIMARY KEY (repo_name, pr_number)
    ) ENGINE=InnoDB
    """)

    c.execute("""
    CREATE TABLE issues (
        repo_name VARCHAR(255) NOT NULL,
        issue_number INT NOT NULL,
        created_at DATETIME NOT NULL,
        closed_at DATETIME,
        first_comment_at DATETIME,
        comments INT,
        creator_login VARCHAR(255),
        updated_at DATETIME,
        PRIMARY KEY (repo_name, issue_number, created_at)
    ) ENGINE=InnoDB
    """)

    c.execute("""
    CREATE TABLE stars (
        repo_name VARCHAR(255) NOT NULL,
        user_login VARCHAR(255),
        starred_at DATETIME NOT NULL,
        PRIMARY KEY (repo_name, starred_at, user_login)
    ) ENGINE=InnoDB
    """)

    conn.commit()
    c.close()
    conn.close()
    print(f"Created tables in DB: {CURRENT_DB_NAME}")

############################
# RETRY logic for requests
############################
def get_session():
    global TOKENS, CURRENT_TOKEN_INDEX
    s = requests.Session()
    if TOKENS:
        current_token = TOKENS[CURRENT_TOKEN_INDEX]
        s.headers.update({"Authorization": f"token {current_token}"})

    retry_strategy = Retry(
        total=5,
        connect=5,
        read=5,
        backoff_factor=2,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["GET", "POST", "PUT", "DELETE", "HEAD", "OPTIONS"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

def maybe_switch_token_if_needed(resp):
    global TOKENS, CURRENT_TOKEN_INDEX
    if not TOKENS:
        return
    rem_str = resp.headers.get("X-RateLimit-Remaining")
    if rem_str:
        try:
            rem_val = int(rem_str)
            if rem_val < MAX_LIMIT_BUFFER and len(TOKENS) > 1:
                old_idx = CURRENT_TOKEN_INDEX
                CURRENT_TOKEN_INDEX = (CURRENT_TOKEN_INDEX + 1) % len(TOKENS)
                print(f"Switching token from {old_idx} to {CURRENT_TOKEN_INDEX} (remaining={rem_val}).")
        except ValueError:
            pass

def handle_rate_limit(resp):
    if resp.status_code == 403:
        reset_time = resp.headers.get("X-RateLimit-Reset")
        if reset_time:
            try:
                reset_ts = int(reset_time)
                now_ts = int(datetime.now().timestamp())
                sleep_seconds = reset_ts - now_ts + 5
                if sleep_seconds > 0:
                    print(f"Rate limit => sleeping {sleep_seconds} sec...")
                    sleep(sleep_seconds)
                    return True
            except ValueError:
                pass
    return False

def handle_rate_limit_and_switch(resp):
    if handle_rate_limit(resp):
        return True
    maybe_switch_token_if_needed(resp)
    return False

############################
# Insert helpers
############################
def db_insert_forks(rows):
    if not rows:
        return
    try:
        conn = connect_db()
        c = conn.cursor()
        sql = """
        INSERT IGNORE INTO forks (repo_name, creator_login, forked_at)
        VALUES (%s, %s, %s)
        """
        data = [(r["repo_name"], r["creator_login"], r["forked_at"]) for r in rows]
        c.executemany(sql, data)
        conn.commit()
        c.close()
        conn.close()
    except Exception as e:
        print(f"DB insert error (forks): {e}")

def db_insert_pulls(rows):
    if not rows:
        return
    try:
        conn = connect_db()
        c = conn.cursor()
        sql = """
        INSERT IGNORE INTO pulls
         (repo_name, pr_number, created_at, first_review_at, merged_at,
          creator_login, title, updated_at)
        VALUES
         (%s, %s, %s, %s, %s, %s, %s, %s)
        """
        data = [
            (r["repo_name"], r["pr_number"], r["created_at"], r["first_review_at"],
             r["merged_at"], r["creator_login"], r["title"], r["updated_at"])
            for r in rows
        ]
        c.executemany(sql, data)
        conn.commit()
        c.close()
        conn.close()
    except Exception as e:
        print(f"DB insert error (pulls): {e}")

def db_insert_issues(rows):
    if not rows:
        return
    try:
        conn = connect_db()
        c = conn.cursor()
        sql = """
        INSERT IGNORE INTO issues
         (repo_name, issue_number, created_at, closed_at, first_comment_at,
          comments, creator_login, updated_at)
        VALUES
         (%s, %s, %s, %s, %s, %s, %s, %s)
        """
        data = [
            (r["repo_name"], r["issue_number"], r["created_at"], r["closed_at"],
             r["first_comment_at"], r["comments"], r["creator_login"], r["updated_at"])
            for r in rows
        ]
        c.executemany(sql, data)
        conn.commit()
        c.close()
        conn.close()
    except Exception as e:
        print(f"DB insert error (issues): {e}")

def db_insert_stars(rows):
    if not rows:
        return
    try:
        conn = connect_db()
        c = conn.cursor()
        sql = """
        INSERT IGNORE INTO stars (repo_name, user_login, starred_at)
        VALUES (%s, %s, %s)
        """
        data = [(r["repo_name"], r["user_login"], r["starred_at"]) for r in rows]
        c.executemany(sql, data)
        conn.commit()
        c.close()
        conn.close()
    except Exception as e:
        print(f"DB insert error (stars): {e}")

############################
# DB coverage queries
############################
def db_get_max_forked_at(repo_name):
    conn = connect_db()
    c = conn.cursor()
    c.execute("SELECT MAX(forked_at) FROM forks WHERE repo_name=%s", (repo_name,))
    row = c.fetchone()
    c.close()
    conn.close()
    return row[0] if row and row[0] else None

def db_get_max_starred_at(repo_name):
    conn = connect_db()
    c = conn.cursor()
    c.execute("SELECT MAX(starred_at) FROM stars WHERE repo_name=%s", (repo_name,))
    row = c.fetchone()
    c.close()
    conn.close()
    return row[0] if row and row[0] else None

def db_get_max_pull_updated(repo_name):
    conn = connect_db()
    c = conn.cursor()
    c.execute("SELECT MAX(updated_at) FROM pulls WHERE repo_name=%s", (repo_name,))
    row = c.fetchone()
    c.close()
    conn.close()
    return row[0] if row and row[0] else None

def db_get_max_issue_updated(repo_name):
    conn = connect_db()
    c = conn.cursor()
    c.execute("SELECT MAX(updated_at) FROM issues WHERE repo_name=%s", (repo_name,))
    row = c.fetchone()
    c.close()
    conn.close()
    return row[0] if row and row[0] else None

############################
# 365-day chunk approach
############################
def parse_date(s):
    if not s:
        return None
    return datetime.strptime(s, "%Y-%m-%d")

def chunk_date_ranges_365(start_dt, end_dt):
    chunks = []
    cur = start_dt
    while cur <= end_dt:
        nxt = cur + timedelta(days=365)
        if nxt > end_dt:
            nxt = end_dt
        chunks.append((cur, nxt))
        cur = nxt + timedelta(days=1)
    return chunks

def get_last_page(resp):
    link_header = resp.headers.get("Link")
    if not link_header:
        return None
    last_page = None
    links = link_header.split(',')
    for link in links:
        match = re.search(r'<([^>]+)>;\s*rel="([^"]+)"', link.strip())
        if match:
            url = match.group(1)
            rel = match.group(2)
            if rel == 'last':
                page_match = re.search(r'[?&]page=(\d+)', url)
                if page_match:
                    try:
                        last_page = int(page_match.group(1))
                    except ValueError:
                        pass
    return last_page

def show_per_entry_progress(table_name, repo_name, item_index, total_items):
    if not total_items or total_items <= 0:
        return
    progress = float(item_index) / float(total_items)
    if progress > 1.0:
        progress = 1.0
    percent = progress * 100
    print(f"[{repo_name}/{table_name}] => {percent:.2f}% done with items...")

############################
# 1) fetch_fork_data => chunk-based + consecutive-empty
############################
def fetch_fork_data(owner, repo, start_str, end_str=None):
    """
    This merges your reference script's fetch logic for forks,
    adding consecutive-empty logic to skip pages early. All debug prints preserved.
    """
    print(f"\n=== fetch_fork_data for {owner}/{repo}, chunk-based ephemeral ===")

    start_dt = parse_date(start_str)
    end_dt   = parse_date(end_str) if end_str else datetime.now()
    if not start_dt or start_dt > end_dt:
        print("No valid date range => done.")
        return

    print(f"Using 365-day chunks, from {start_dt.date()} -> {end_dt.date()}")
    all_chunks = chunk_date_ranges_365(start_dt, end_dt)
    if not all_chunks:
        print("No chunks => done.")
        return

    repo_name = f"{owner}/{repo}"
    db_max_dt = db_get_max_forked_at(repo_name)
    table_name = "forks"

    total_chunks = len(all_chunks)
    for i, (chunk_start, chunk_end) in enumerate(all_chunks, start=1):
        done_percent = (i / total_chunks) * 100
        left_percent = 100 - done_percent
        print(f"[{table_name.upper()}] chunk {i}/{total_chunks}: about {left_percent:.1f}% left to finish chunk approach.")
        print(f"    chunk range: {chunk_start.date()} => {chunk_end.date()} (365-day chunk)")

        page = 1
        consecutive_empty_pages = 0
        MAX_EMPTY_PAGES = 5

        while True:
            session = get_session()
            url = f"https://api.github.com/repos/{owner}/{repo}/forks"
            params = {
                "page": page,
                "per_page": 100,
                "sort": "oldest",
                "direction": "asc"
            }
            resp = session.get(url, params=params)
            if handle_rate_limit_and_switch(resp):
                continue

            if resp.status_code != 200:
                print(f"[{table_name.upper()}] HTTP {resp.status_code}, stop page={page}")
                break

            data = resp.json()
            if not data:
                print(f"    page={page}: no data => end of chunk.")
                break

            last_page = get_last_page(resp)
            total_items = last_page * 100 if last_page else None

            skip_chunk = False
            new_rows = []
            for idx, fork_info in enumerate(data):
                dt_str = fork_info.get("created_at")
                if not dt_str:
                    continue
                forked_dt = datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%SZ")

                # partial skip => if older => skip remainder
                if db_max_dt and forked_dt <= db_max_dt:
                    print(f"    partial skip => older item {forked_dt} <= db_max_dt={db_max_dt}")
                    skip_chunk = True
                    break

                # check if forked_dt out of chunk range
                if forked_dt < chunk_start or forked_dt > chunk_end:
                    # outside this chunk => skip
                    if forked_dt > chunk_end:
                        print(f"    fork {forked_dt} > chunk_end => skip chunk.")
                        skip_chunk = True
                    # else it's older than chunk_start => ignore
                    break

                new_rows.append({
                    "repo_name": repo_name,
                    "creator_login": fork_info.get("owner", {}).get("login", ""),
                    "forked_at": forked_dt
                })

                # per-entry progress
                item_index = (page - 1) * 100 + (idx + 1)
                show_per_entry_progress(table_name, repo_name, item_index, total_items)

            db_insert_forks(new_rows)
            print(f"    page={page}: inserted={len(new_rows)} forks (out of {len(data)}).")

            if len(new_rows) == 0:
                consecutive_empty_pages += 1
            else:
                consecutive_empty_pages = 0

            if skip_chunk:
                break

            if len(data) < 100:
                print("    last page => break.")
                break
            if consecutive_empty_pages >= MAX_EMPTY_PAGES:
                print(f"    {consecutive_empty_pages} consecutive empty pages => break chunk.")
                break

            page += 1
            print(f"    next page => {page}")

############################
# 2) fetch_issue_data => incremental
############################
def fetch_issue_data(owner, repo, start_str, end_str=None):
    """
    Merges your reference script's logic for issues with incremental ?since=...
    All debug prints retained.
    """
    print(f"\n=== fetch_issue_data for {owner}/{repo}, from {start_str} -> {end_str or 'NOW'} ===")

    start_dt = parse_date(start_str)
    end_dt   = parse_date(end_str) if end_str else datetime.now()
    if not start_dt or start_dt > end_dt:
        print("No valid date range => done.")
        return

    repo_name = f"{owner}/{repo}"
    db_max_update = db_get_max_issue_updated(repo_name)
    table_name = "issues"
    if db_max_update:
        print(f"Already have issues updated up to: {db_max_update}")
    else:
        print("No DB data yet => full fetch from start_dt.")

    # convert to iso string if db_max_update exists, else fallback to start_date
    since_dt = db_max_update if db_max_update else start_dt
    since_str = since_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    page = 1
    while True:
        session = get_session()
        url = f"https://api.github.com/repos/{owner}/{repo}/issues"
        params = {
            "state": "all",
            "sort": "updated",
            "direction": "asc",
            "page": page,
            "per_page": 50,
            "since": since_str
        }
        resp = session.get(url, params=params)
        if handle_rate_limit_and_switch(resp):
            continue

        if resp.status_code != 200:
            print(f"[{table_name.upper()}] HTTP {resp.status_code}, stop page={page}")
            break

        data = resp.json()
        if not data:
            print(f"    page={page}: no data => end of incremental fetch.")
            break

        last_page = get_last_page(resp)
        total_items = last_page * 50 if last_page else None

        new_rows = []
        skip_chunk = False
        for idx, issue in enumerate(data):
            if "pull_request" in issue:
                continue
            updated_str = issue.get("updated_at")
            if not updated_str:
                continue
            updated_dt = datetime.strptime(updated_str, "%Y-%m-%dT%H:%M:%SZ")

            if db_max_update and updated_dt <= db_max_update:
                print(f"    partial skip => older item {updated_dt} <= db_max_update={db_max_update}")
                skip_chunk = True
                break

            created_str = issue.get("created_at")
            created_dt = datetime.strptime(created_str, "%Y-%m-%dT%H:%M:%SZ") if created_str else None
            closed_str = issue.get("closed_at")
            closed_dt = datetime.strptime(closed_str, "%Y-%m-%dT%H:%M:%SZ") if closed_str else None
            comments_count = issue.get("comments", 0)
            creator_login = issue.get("user", {}).get("login", "")
            issue_num = issue["number"]

            new_rows.append({
                "repo_name": repo_name,
                "issue_number": issue_num,
                "created_at": created_dt,
                "closed_at": closed_dt,
                "first_comment_at": None,
                "comments": comments_count,
                "creator_login": creator_login,
                "updated_at": updated_dt
            })

            item_index = (page - 1) * 50 + (idx + 1)
            show_per_entry_progress(table_name, repo_name, item_index, total_items)

        db_insert_issues(new_rows)
        print(f"    page={page}: inserted={len(new_rows)} issues (out of {len(data)}).")

        if skip_chunk:
            break

        if len(data) < 50:
            print("    last page => break.")
            break
        page += 1
        print(f"    next page => {page}")

############################
# 3) fetch_pull_data => incremental
############################
def fetch_pull_data(owner, repo, start_str, end_str=None):
    """
    Merges your reference logic for pulls, but fetch them from /issues with pull_request key.
    All debug prints retained.
    """
    print(f"\n=== fetch_pull_data for {owner}/{repo}, from {start_str} -> {end_str or 'NOW'} ===")

    start_dt = parse_date(start_str)
    end_dt   = parse_date(end_str) if end_str else datetime.now()
    if not start_dt or start_dt > end_dt:
        print("No valid date range => done.")
        return

    repo_name = f"{owner}/{repo}"
    db_max_update = db_get_max_pull_updated(repo_name)
    table_name = "pulls"
    if db_max_update:
        print(f"Already have pulls updated up to: {db_max_update}")
    else:
        print("No DB data yet => full fetch from start_dt.")

    since_dt = db_max_update if db_max_update else start_dt
    since_str = since_dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    page = 1
    while True:
        session = get_session()
        url = f"https://api.github.com/repos/{owner}/{repo}/issues"
        params = {
            "state": "all",
            "sort": "updated",
            "direction": "asc",
            "page": page,
            "per_page": 50,
            "since": since_str
        }
        resp = session.get(url, params=params)
        if handle_rate_limit_and_switch(resp):
            continue

        if resp.status_code != 200:
            print(f"[{table_name.upper()}] HTTP {resp.status_code}, stop page={page}")
            break

        data = resp.json()
        if not data:
            print(f"    page={page}: no data => end of incremental fetch.")
            break

        last_page = get_last_page(resp)
        total_items = last_page * 50 if last_page else None

        new_rows = []
        skip_chunk = False
        for idx, issue_like in enumerate(data):
            if "pull_request" not in issue_like:
                continue
            updated_str = issue_like.get("updated_at")
            if not updated_str:
                continue
            updated_dt = datetime.strptime(updated_str, "%Y-%m-%dT%H:%M:%SZ")

            if db_max_update and updated_dt <= db_max_update:
                print(f"    partial skip => older item {updated_dt} <= db_max_update={db_max_update}")
                skip_chunk = True
                break

            created_str = issue_like.get("created_at")
            created_dt = datetime.strptime(created_str, "%Y-%m-%dT%H:%M:%SZ") if created_str else None
            merged_str = issue_like.get("closed_at")  # or we do a separate call
            merged_dt  = datetime.strptime(merged_str, "%Y-%m-%dT%H:%M:%SZ") if merged_str else None

            pr_number = issue_like["number"]
            user_login = issue_like.get("user", {}).get("login", "")
            title = issue_like.get("title", "")

            new_rows.append({
                "repo_name": repo_name,
                "pr_number": pr_number,
                "created_at": created_dt,
                "first_review_at": None,
                "merged_at": merged_dt,
                "creator_login": user_login,
                "title": title,
                "updated_at": updated_dt
            })

            item_index = (page - 1) * 50 + (idx + 1)
            show_per_entry_progress(table_name, repo_name, item_index, total_items)

        db_insert_pulls(new_rows)
        print(f"    page={page}: inserted={len(new_rows)} pulls (out of {len(data)}).")

        if skip_chunk:
            break

        if len(data) < 50:
            print("    last page => break.")
            break
        page += 1
        print(f"    next page => {page}")

############################
# 4) fetch_star_data => chunk-based + consecutive-empty
############################
def fetch_star_data(owner, repo, start_str, end_str=None):
    """
    This merges your reference script's logic for stargazers, adding consecutive-empty.
    All debug prints preserved.
    """
    print(f"\n=== fetch_star_data for {owner}/{repo}, from {start_str} -> {end_str or 'NOW'} ===")

    start_dt = parse_date(start_str)
    end_dt   = parse_date(end_str) if end_str else datetime.now()
    if not start_dt or start_dt > end_dt:
        print("No valid date range => done.")
        return

    print(f"Using 365-day chunks for stargazers, from {start_dt.date()} -> {end_dt.date()}")
    all_chunks = chunk_date_ranges_365(start_dt, end_dt)
    if not all_chunks:
        print("No chunks => done.")
        return

    repo_name = f"{owner}/{repo}"
    db_max_dt = db_get_max_starred_at(repo_name)
    table_name = "stars"

    total_chunks = len(all_chunks)
    for i, (chunk_start, chunk_end) in enumerate(all_chunks, start=1):
        done_percent = (i / total_chunks) * 100
        left_percent = 100 - done_percent
        print(f"[{table_name.upper()}] chunk {i}/{total_chunks}: about {left_percent:.1f}% left to finish the chunk approach.")
        print(f"    chunk range: {chunk_start.date()} => {chunk_end.date()} (365-day chunk)")

        page = 1
        consecutive_empty_pages = 0
        MAX_EMPTY_PAGES = 5

        while True:
            session = get_session()
            session.headers["Accept"] = "application/vnd.github.v3.star+json"
            url = f"https://api.github.com/repos/{owner}/{repo}/stargazers"
            params = {
                "page": page,
                "per_page": 100
            }
            resp = session.get(url, params=params)
            if handle_rate_limit_and_switch(resp):
                continue

            if resp.status_code != 200:
                print(f"[{table_name.upper()}] HTTP {resp.status_code}, stop page={page}")
                break

            data = resp.json()
            if not data:
                print(f"    page={page}: no data => end of chunk.")
                break

            last_page = get_last_page(resp)
            total_items = last_page * 100 if last_page else None

            skip_chunk = False
            new_rows = []
            for idx, star_info in enumerate(data):
                starred_str = star_info.get("starred_at")
                if not starred_str:
                    continue
                starred_dt = datetime.strptime(starred_str, "%Y-%m-%dT%H:%M:%SZ")

                # partial skip => if older => skip remainder
                if db_max_dt and starred_dt <= db_max_dt:
                    print(f"    partial skip => older item {starred_dt} <= db_max_dt={db_max_dt}")
                    skip_chunk = True
                    break

                # check if starred_dt is outside chunk range
                if starred_dt < chunk_start or starred_dt > chunk_end:
                    if starred_dt > chunk_end:
                        print(f"    star {starred_dt} > chunk_end => skip chunk.")
                        skip_chunk = True
                    break

                user_login = star_info.get("user", {}).get("login", "")
                new_rows.append({
                    "repo_name": repo_name,
                    "user_login": user_login,
                    "starred_at": starred_dt
                })

                # per-entry progress
                item_index = (page - 1) * 100 + (idx + 1)
                show_per_entry_progress(table_name, repo_name, item_index, total_items)

            db_insert_stars(new_rows)
            print(f"    page={page}: inserted={len(new_rows)} stars (out of {len(data)}).")

            if len(new_rows) == 0:
                consecutive_empty_pages += 1
            else:
                consecutive_empty_pages = 0

            if skip_chunk:
                break

            if len(data) < 100:
                print("    last page => break.")
                break
            if consecutive_empty_pages >= MAX_EMPTY_PAGES:
                print(f"    {consecutive_empty_pages} consecutive empty pages => break chunk.")
                break

            page += 1
            print(f"    next page => {page}")

if __name__ == "__main__":
    load_tokens()
    create_and_select_db()
    create_tables()
    print(f"New DB => {CURRENT_DB_NAME}. Now call fetch_*_data from 'caller.py'.")
3) caller.py
Finally, the orchestrator that:

Loads tokens.
Creates a brand-new DB each run.
Builds ephemeral tables.
Loops over repos from repo_list.py, calling chunk-based forks/stars with consecutive-empty logic, and incremental issues/pulls logic with debug prints.
python
Copy code
#!/usr/bin/env python
# caller.py

from repo_list import repo_list
from fetch_data import (
    load_tokens,
    create_and_select_db,
    create_tables,
    fetch_fork_data,
    fetch_issue_data,
    fetch_pull_data,
    fetch_star_data,
    CURRENT_DB_NAME
)

def main():
    # 1) Load GitHub tokens
    load_tokens()

    # 2) Create brand-new DB => ephemeral approach
    create_and_select_db()

    # 3) Create tables in that new DB
    create_tables()

    print(f"\nUsing newly created DB: {CURRENT_DB_NAME}\n")

    # 4) For each repo
    for repo_info in repo_list:
        if not repo_info.get("enabled", False):
            continue

        owner = repo_info["owner"]
        repo  = repo_info["repo"]
        start_str = repo_info["start_date"]
        end_str   = repo_info["end_date"] or None

        print("\n========================================")
        print(f"Processing {owner}/{repo} from {start_str} to {end_str or 'NOW'}")

        # chunk-based => forks & stars
        fetch_fork_data(owner, repo, start_str, end_str)
        fetch_star_data(owner, repo, start_str, end_str)

        # incremental => issues & pulls
        fetch_issue_data(owner, repo, start_str, end_str)
        fetch_pull_data(owner, repo, start_str, end_str)

    print("\nAll done! Data stored in DB:", CURRENT_DB_NAME)

if __name__ == "__main__":
    main()
Caveats
CREATE DATABASE Permission:

Your MySQL user must have permission to create new databases. If not, an error will occur.
Incremental vs. Fresh:

Each run uses a brand-new DB, so the “incremental” approach effectively does a full fetch from start_date.
We keep the debug prints referencing db_max_dt or db_max_update, but it will generally be None for a new DB.
Many DBs Over Time:

If you run frequently, you’ll accumulate many DBs named like my_kpis_db_2023_08_21_153045. You may want to eventually drop old ones or rename them for archiving.
Consecutive Empty Pages:

MAX_EMPTY_PAGES = 5. If you get 5 consecutive pages with zero new inserts, the script skips the rest of that chunk. This speeds up chunk-based fetches but relies on ascending date order from GitHub.
No Merging of Data**:

Because you create a new DB each time, there’s no merging with old data. You keep a fresh snapshot run.
Final Description (So Another AI Can Recreate)
repo_list.py: A small file listing the repos as dictionaries (with fields like owner, repo, start_date, end_date).
fetch_data.py:
Creates a unique DB name each run (create_and_select_db()).
Creates ephemeral tables in that DB (create_tables()).
Has chunk-based logic for forks/stars with a “stop early if N consecutive empty pages” approach plus all debug prints from your reference script.
Has incremental logic for issues/pulls using ?since=..., also with your debug prints.
Uses a robust “Requests” session to handle rate limits, partial skipping, token switching, etc.
caller.py:
Loads tokens, calls create_and_select_db(), create_tables().
Loops over each repo_info in repo_list, calling the relevant fetch_*_data functions.
Each run yields a brand-new DB containing the chunk-based forks/stars and incremental issues/pulls data.
By copying the code above into these three Python files, you have the entire solution with all your debug prints (page=..., inserted=..., partial skip =>, etc.), consecutive-empty logic for chunk-based endpoints, and incremental for issues/pulls.